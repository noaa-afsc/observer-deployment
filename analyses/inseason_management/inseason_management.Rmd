---
title: "Evaluating Fisheries Data Quality for Inseason Management"
author: ""
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
# Get packages ----
library(data.table)
library(flextable)
library(gridExtra)
library(lubridate)
library(pals)
library(ROracle)
library(scales)
library(tidyverse)

# Set random number seed ----
set.seed(49)

# Load data ----
load("analyses/inseason_management/valhalla_accounts.RData")  

# Isolate the data used for most of the analysis ----
valhalla_2021 <- valhalla %>% 
                 filter(ADP == 2021 & ACCOUNT_NAME != "No Account")

# Most recent full year ----
year <- year(Sys.Date()) - 1

# Figure theme ----
fig_theme <- list(theme_classic(), scale_y_continuous(labels = comma), scale_fill_manual(values = kelly()[2:22]))

# Do not print code chunks by default ----
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction
One of the purposes for which fishery-dependent data are used is inseason management, broadly defined as the opening and closing of fisheries throughout the year based on the proportion of total allowable catch (TAC) that has been harvested or the proportion of prohibited species catch (PSC) that has been caught, relative to PSC limits. Catch that accrues toward TAC or PSC limits is assigned by the Catch Accounting System (CAS) to different accounts. These accounts are the units within which catch is tracked by inseason management, and so they will form the basis of this analysis. This is a table of all the accounts used in 2021 for catch contained within the Valhalla data set: 
```{r}
matrix(sort(unique(valhalla_2021$ACCOUNT_NAME)), ncol = 5) %>% 
data.frame() %>% 
flextable() %>% 
fontsize(size = 7) %>% 
delete_part(part = "header") %>% 
width(width = 4) %>% 
padding(padding = 1)
```

# The Goal of This Analysis
The goal of this analysis is to identify a metric that can be used to differentiate the performance of different sampling designs with regards to how well they provide data for inseason management. It's important to note that the goal is *not* to identify the absolute best metric for measuring design performance for inseason management; the metric only needs to be accurate enough to differentiate between the performance of different sampling designs. The goal is also *not* to quantify how much data is needed for inseason management.

# What's a Good Sampling Design for Inseason Mangement?
Catch is either retained or discarded. Retained catch is known, whereas discarded catch is estimated. With more precise estimates, an inseason manager is able to close a fishery closer to its TAC or PSC limit. Discard estimates are made using data from fisheries observers or electronic monitoring (EM). More monitoring data (from observers or EM) won't benefit inseason management if it's directed toward a fishery that retains all catch. Therefore, a sampling design that dedicates more sampling toward fisheries with a lot of highly variable discards will outperform a design that puts more sampling toward fisheries with small amounts of discards with little variance.

It's therefore helpful to visualize how much of the catch in each account is made up of discards:  
```{r accounts_discard}
accounts_discard <- valhalla_2021 %>% 
                    mutate(DISCARD_FLAG = recode(SOURCE_TABLE, 'Y' = 'N', 'N' = 'Y')) %>% 
                    group_by(ACCOUNT_NAME, DISCARD_FLAG) %>%
                    summarise(weight = sum(WEIGHT_POSTED, na.rm = TRUE),
                              count  = sum(SPECIES_COUNT, na.rm = TRUE),
                              .groups = 'drop') %>% 
                    mutate(catch = ifelse(weight > 0, weight, count)) %>% 
                    group_by(ACCOUNT_NAME) %>% 
                    mutate(percentage = catch / sum(catch) * 100) %>% 
                    arrange(DISCARD_FLAG, desc(percentage)) %>% 
                    ungroup()

f1 <- ggplot(accounts_discard, aes(x = ACCOUNT_NAME, y = percentage, fill = DISCARD_FLAG)) +
      geom_col() +
      scale_x_discrete(limits = unique(accounts_discard$ACCOUNT_NAME[order(desc(accounts_discard$DISCARD_FLAG), desc(accounts_discard$percentage))])) +
      scale_fill_manual(values = kelly()[2:22]) +
      labs(x = "Account Name", y = "Percentage of Catch", fill = "Discard") +
      coord_flip() +
      theme_classic() +
      theme(axis.text.y = element_text(size = 4), legend.position = "bottom")

f1
```

It's also helpful to visualize how much catch occurs in partial coverage within each account:  
```{r accounts_coverage}
accounts_coverage <- valhalla_2021 %>% 
                     group_by(ACCOUNT_NAME, COVERAGE_TYPE) %>%
                     summarise(weight = sum(WEIGHT_POSTED, na.rm = TRUE),
                               count  = sum(SPECIES_COUNT, na.rm = TRUE),
                               .groups = 'drop') %>% 
                     mutate(catch = ifelse(weight > 0, weight, count)) %>% 
                     group_by(ACCOUNT_NAME) %>% 
                     mutate(percentage = catch / sum(catch) * 100) %>% 
                     arrange(COVERAGE_TYPE, desc(percentage)) %>% 
                     ungroup()

f2 <- ggplot(accounts_coverage, aes(x = ACCOUNT_NAME, y = percentage, fill = COVERAGE_TYPE)) +
      geom_col() +
      scale_x_discrete(limits = unique(accounts_coverage$ACCOUNT_NAME[order(desc(accounts_coverage$COVERAGE_TYPE), desc(accounts_coverage$percentage))])) +
      scale_fill_manual(values = kelly()[2:22], labels = c("Full", "Partial")) +
      labs(x = "Account Name", y = "Percentage of Catch", fill = "Coverage") +
      coord_flip() +
      theme_classic() +
      theme(axis.text.y = element_text(size = 4), legend.position = "bottom")

f2
```

Although the patterns in these plots are very similar, the account names on the y-axes are not in the same order. Let's look at the graphs side by side with the same account order:  
```{r accounts_discard_coverage}
f3 <- ggplot(accounts_coverage, aes(x = ACCOUNT_NAME, y = percentage, fill = COVERAGE_TYPE)) +
      geom_col() +
      scale_x_discrete(limits = unique(accounts_discard$ACCOUNT_NAME[order(desc(accounts_discard$DISCARD_FLAG), desc(accounts_discard$percentage))])) +
      scale_fill_manual(values = kelly()[2:22], labels = c("Full", "Partial")) +
      labs(x = "Account Name", y = "Percentage of Catch", fill = "Coverage") +
      coord_flip() +
      theme_classic() +
      theme(axis.text.y = element_text(size = 4), legend.position = "bottom", legend.title = element_text(size=8), legend.text = element_text(size=6))

grid.arrange(f1, f3, ncol = 2)
```

This shows that accounts with a high proportion of discards aren't necessarily in full coverage. There are several accounts in which all catch was discarded and fell within partial coverage. But we aren't interested in discard *amount*, we are interested in discard *variability*, so let's look at the variability of discards that occurred on observed trips relative to realized coverage rates within each account:
```{r, warning=FALSE}
accounts_se <- valhalla_2021 %>% 
               group_by(ACCOUNT_NAME, TRIP_ID, OBSERVED_FLAG) %>% 
               summarise(discard = sum(WEIGHT_POSTED[SOURCE_TABLE == "N"], na.rm = TRUE), .groups = 'drop') %>% 
               group_by(ACCOUNT_NAME) %>% 
               summarise(N = n_distinct(TRIP_ID),
                         n = n_distinct(TRIP_ID[OBSERVED_FLAG == "Y"]),
                         sd_trip = sd(discard[OBSERVED_FLAG == "Y"]),
                         se_trip = sqrt((N-n)/N) * sqrt(1/n) * sd_trip,
                         se_account = se_trip * N)

f4 <- ggplot(accounts_se, aes(x = n / N * 100, y = sd_trip, label = ACCOUNT_NAME)) +
      geom_label(size = 2.5) +
      labs(x = "Monitoring Rate (%)", y = "Standard Deviation of Trip-Level Discards") +
      lims(y = c(0, 70)) +
      theme_classic()

f4
```

There's clearly an outlier in BS Pollock Incidental Catch Allowance (ICA), but standard deviation isn't quite what we're after anyway, because highly variable discards at the trip level would still be precisely estimated if monitoring rates are high. We're interested in the standard deviation of the *estimate*. In this case, the estimate of the amount of discards that occurred within the account. The standard deviation of an estimate is called standard error, and we can begin by first calculating the standard error of the mean amount of discards that occurred on each trip within an account by using the following equation:

<center>

$SE_{t} = \sqrt{\frac{N - n}{N} \frac{1}{n} \frac{\sum_{i=1}^{n} (d_{i}-\bar{d})^2}{n-1}}$

</center>

Where $N$ is the total number of trips that occurred within an account, $n$ is the total number of monitored trips that occurred within an account, each $d_{i}$ is the amount of discards that occurred on a monitored trip, and $\bar{d}$ is the mean amount of discards that occurred on monitored trips within that account. We use the finite population correction factor ($\frac{N - n}{N}$) to account for the proportion of the population being sampled. The standard error of the estimate of the total amount of catch that occurred with in an account is this trip-level standard error multiplied by the total number of trips in that account:  

<center>

$SE_{a} = SE_{t}N$

</center>

This plot shows the account-level standard error against monitoring rate for each account:
```{r, warning=FALSE}
f6 <- ggplot(accounts_se, aes(x = n / N * 100, y = se_account, label = ACCOUNT_NAME)) +
      geom_label(size = 2.5) +
      labs(x = "Monitoring Rate (%)", y = "Standard Error of Account-Level Discard Estimate") +
      fig_theme

f6
```

It might be more useful to look at the distribution of standard errors: 
```{r, warning=FALSE}
f7 <- ggplot(accounts_se, aes(x = se_account)) +
      geom_histogram(bins = 30) +
      geom_vline(xintercept = median(accounts_se$se_account, na.rm = TRUE), lty = 2, color = "red") +
      labs(x = "Standard Error of Account-Level Discard Estimate", y = "Number of Accounts") +
      fig_theme +
      theme(plot.title = element_text(hjust = 0.5))

f7
```

The dashed red line represents the median standard error across accounts. A sampling design with a lower median standard error would be better for inseason management since it would mean that on average, the amount of discards that occurred within any given account were estimated with more precision. So far this analysis has been using 2021 data and the actual sampling that occurred in that year. All monitored trips were counted, regardless of whether they were monitored by an observer or EM. However, when the 2024 analytic team met with inseason managers, a consistent message from them was that fixed-gear EM isn't currently very useful for inseason management, since data often come in weeks or months after the trip occurred. As a test of median standard error as a metric for differentiating between sample design performance for inseason management, we could compare what actually occurred in 2021 with a mock data set that only counts 25% of fixed-gear trips monitored with EM as having been monitored. This number was chosen based on inseason manager's assessment that 1 week is about the maximum turnaround time EM data could have to still be useful for inseason management and the fact that the 2020 Annual Report (AFSC and AKRO 2021) showed that even the 25th percentile of review times (from hard drive received to data exported) was 9 days and 16 days for hook-and-line and pot gear, respectively. Comparing the mock design that only counts 25% of the monitored fixed-gear EM trips against what actually occurred in 2021, we get this:
```{r, warning=FALSE}
monitored_fgem_trips <- valhalla_2021 %>% 
                        filter(STRATA %in% c("EM_HAL", "EM_POT") & OBSERVED_FLAG == "Y") %>% 
                        distinct(TRIP_ID) %>% 
                        unlist() %>% 
                        as.vector()
  
accounts_se_mock <- valhalla_2021 %>% 
                    mutate(OBSERVED_FLAG = ifelse(STRATA %in% c("EM_HAL", "EM_POT"), "N", OBSERVED_FLAG)) %>% 
                    full_join(data.frame(rep = c(1:5)), by = character()) %>% 
                    group_by(rep) %>% 
                    mutate(OBSERVED_FLAG = ifelse(STRATA %in% c("EM_HAL", "EM_POT") & TRIP_ID %in% sample(monitored_fgem_trips, round(length(monitored_fgem_trips) * 0.25)), "Y", OBSERVED_FLAG)) %>%
                    group_by(ACCOUNT_NAME, TRIP_ID, OBSERVED_FLAG, rep) %>% 
                    summarise(discard = sum(WEIGHT_POSTED[SOURCE_TABLE == "N"], na.rm = TRUE), .groups = 'drop') %>% 
                    group_by(ACCOUNT_NAME, rep) %>% 
                    summarise(N = n_distinct(TRIP_ID),
                              n = n_distinct(TRIP_ID[OBSERVED_FLAG == "Y"]),
                              sd_trip = sd(discard[OBSERVED_FLAG == "Y"]),
                              se_trip = sqrt((N-n)/N) * sqrt(1/n) * sd_trip,
                              se_account = se_trip * N, 
                              .groups = 'drop')

f8 <- ggplot(accounts_se_mock %>% filter(rep == 1), aes(x = se_account)) +
      geom_histogram(bins = 30) +
      geom_vline(xintercept = median(accounts_se_mock$se_account, na.rm = TRUE), lty = 2, color = "red") +
      labs(title = "Fixed-Gear EM Monitoring Reduced by 75%", x = "Standard Error of Account-Level Discard Estimate", y = "Number of Accounts") +
      fig_theme +
      theme(plot.title = element_text(hjust = 0.5))

grid.arrange(f7 + labs(title = "Original Data"), f8, ncol = 1)
```

It's hard to see a difference in medians from the graph, so here they are in table form:
```{r}
rbind(accounts_se %>% mutate(Design = "Original"),
      accounts_se_mock %>% filter(rep == 1) %>% select(-rep) %>% mutate(Design = "Reduced EM")) %>% 
group_by(Design) %>% 
summarise(`Median Standard Error` = round(median(se_account, na.rm = TRUE), 2)) %>% 
flextable() %>% 
autofit()
```

The mock design used a random draw of monitored fixed-gear EM trips, and we can see that repeating that process consistently results in median standard errors that are higher than what actually occurred in 2021:
```{r}
rbind(accounts_se %>% mutate(Design = "Original"),
      accounts_se_mock %>% filter(rep == 1) %>% select(-rep) %>% mutate(Design = "Reduced EM 1"),
      accounts_se_mock %>% filter(rep == 2) %>% select(-rep) %>% mutate(Design = "Reduced EM 2"),
      accounts_se_mock %>% filter(rep == 3) %>% select(-rep) %>% mutate(Design = "Reduced EM 3"),
      accounts_se_mock %>% filter(rep == 4) %>% select(-rep) %>% mutate(Design = "Reduced EM 4"),
      accounts_se_mock %>% filter(rep == 5) %>% select(-rep) %>% mutate(Design = "Reduced EM 5")) %>% 
group_by(Design) %>% 
summarise(`Median Standard Error` = round(median(se_account, na.rm = TRUE), 2)) %>% 
flextable() %>% 
autofit()
```

This tells us that if all fixed-gear EM data were usable for inseason management, that design would perform better than one in which only 25% of fixed-gear EM data are usable for inseason management. Median standard error is the metric being proposed for differentiating the performance of different sampling designs with regards to providing data for inseason management.  

# Potential Criticisms of This Approach  
1. The standard error estimates are wrong because they assume that catch is known with perfect accuracy at the trip-level.    
2. Trip counts are wrong because estimates were included when counting the number of trips that contributed catch to an account, and those estimates were made based on current CAS post-stratification rules.    

# Responses to Potential Criticisms  
1. Assuming catch is known at the trip-level is an intentional simplification. That simplification prevents the standard error estimates from being published as accurate, but it does not prevent this approach from being used to differentiate between the performance of designs.  
2. Total catch estimates are our best approximation of what catch occurred on each trip. If post-stratification methods provide more precise total catch estimates in the future, it will only benefit the approach used here.  

# Open Questions
1. Should accounts with PSC limits be treated differently than accounts without PSC limits?    

# References  
AFSC (Alaska Fisheries Science Center) and AKRO (Alaska Regional Office). 2021. North Pacific Observer Program 2020 Annual Report. AFSC Processed Rep. 2021-03, 143 p. Alaska Fish. Sci. Cent., NOAA, Natl. Mar. Fish. Serv., 7600 Sand Point Way NE, Seattle WA 98115.Available at https://repository.library.noaa.gov/view/noaa/30732.